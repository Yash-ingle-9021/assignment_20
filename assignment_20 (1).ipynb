{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf427f2-7c37-4a54-bd22-72e9b0846ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ASSIGNMENT_20 *\n"
     ]
    }
   ],
   "source": [
    "print(\"* ASSIGNMENT_20 *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e19623-d173-4d4b-8d97-e6062d69d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1_ANS :- Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data. Web Scraping has multiple applications across various industries. Let’s check out some of these now\n",
      " 1. Price Monitoring\n",
      "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
      "2. Market Research\n",
      "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future.\n",
      " 3. News Monitoring\n",
      "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
      "4. Sentiment Analysis\n",
      "If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\n",
      "5. Email Marketing\n",
      "Companies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s.\n",
      "Web scraping tools can help gather relevant data from the web at a quick rate and for extended periods of time. For example, if you're gathering data around a trending word, you may use a web scraping tool that gathers data only when social media users use that word in a hashtag format or in a heading. This can help you automatically filter through content to find what you need. You can also set a web scraping tool to gather data even when you're not at your computer. This may help you complete extensive searches.\n",
      "There are a variety of uses you can gain from web scraping, including:\n",
      "Monitoring e-commerce prices\n",
      "Finding opportunities for investment\n",
      "Analyzing social media web data\n",
      "Applying machine learning techniques\n",
      "Gathering web data automatically\n",
      "Researching new concepts in a field\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_1_ANS :- Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data. Web Scraping has multiple applications across various industries. Let’s check out some of these now\\n 1. Price Monitoring\\nWeb Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\\n2. Market Research\\nWeb scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future.\\n 3. News Monitoring\\nWeb scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\\n4. Sentiment Analysis\\nIf companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\\n5. Email Marketing\\nCompanies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s.\\nWeb scraping tools can help gather relevant data from the web at a quick rate and for extended periods of time. For example, if you're gathering data around a trending word, you may use a web scraping tool that gathers data only when social media users use that word in a hashtag format or in a heading. This can help you automatically filter through content to find what you need. You can also set a web scraping tool to gather data even when you're not at your computer. This may help you complete extensive searches.\\nThere are a variety of uses you can gain from web scraping, including:\\nMonitoring e-commerce prices\\nFinding opportunities for investment\\nAnalyzing social media web data\\nApplying machine learning techniques\\nGathering web data automatically\\nResearching new concepts in a field\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7463d4-2c99-4473-8c9f-ddfae5b3e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_2_ANS :- The administrator of a website can use various measures to stop or slow a bot. Some techniques include:\n",
      "Blocking an IP address either manually or based on criteria such as geolocation and DNSRBL. This will also block all browsing from that address.\n",
      "Disabling any web service API that the website's system might expose.\n",
      "Bots sometimes declare who they are (using user agent strings) and can be blocked on that basis using robots.txt; 'googlebot' is an example. Other bots make no distinction between themselves and a human using a browser.\n",
      "Bots can be blocked by monitoring excess traffic\n",
      "Bots can sometimes be blocked with tools to verify that it is a real person accessing the site, like a CAPTCHA. Bots are sometimes coded to explicitly break specific CAPTCHA patterns or may employ third-party services that utilize human labor to read and respond in real-time to CAPTCHA challenges.\n",
      "Commercial anti-bot services: Companies offer anti-bot and anti-scraping services for websites. A few web application firewalls have limited bot detection capabilities as well. However, many such solutions are not very effective.[28]Locating bots with a honeypot or other method to identify the IP addresses of automated crawlers.\n",
      "Obfuscation using CSS sprites to display such data as telephone numbers or email addresses, at the cost of accessibility to screen reader users.\n",
      "Because bots rely on consistency in the front-end code of a target website, adding small variations to the HTML/CSS surrounding important data and navigation elements would require more human involvement in the initial set up of a bot and if done effectively may render the target website too difficult to scrape due to the diminished ability to automate the scraping process.\n",
      "Websites can declare if crawling is allowed or not in the robots.txt file and allow partial access, limit the crawl rate, specify the optimal time to crawl and more.\n",
      "Load database data straight into the HTML DOM via AJAX, and use DOM methods to display it, forcing crawlers to either reproduce those AJAX requests or use browser rendering (e.g. a headless browser).\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Q_2_ANS :- The administrator of a website can use various measures to stop or slow a bot. Some techniques include:\\nBlocking an IP address either manually or based on criteria such as geolocation and DNSRBL. This will also block all browsing from that address.\\nDisabling any web service API that the website's system might expose.\\nBots sometimes declare who they are (using user agent strings) and can be blocked on that basis using robots.txt; 'googlebot' is an example. Other bots make no distinction between themselves and a human using a browser.\\nBots can be blocked by monitoring excess traffic\\nBots can sometimes be blocked with tools to verify that it is a real person accessing the site, like a CAPTCHA. Bots are sometimes coded to explicitly break specific CAPTCHA patterns or may employ third-party services that utilize human labor to read and respond in real-time to CAPTCHA challenges.\\nCommercial anti-bot services: Companies offer anti-bot and anti-scraping services for websites. A few web application firewalls have limited bot detection capabilities as well. However, many such solutions are not very effective.[28]Locating bots with a honeypot or other method to identify the IP addresses of automated crawlers.\\nObfuscation using CSS sprites to display such data as telephone numbers or email addresses, at the cost of accessibility to screen reader users.\\nBecause bots rely on consistency in the front-end code of a target website, adding small variations to the HTML/CSS surrounding important data and navigation elements would require more human involvement in the initial set up of a bot and if done effectively may render the target website too difficult to scrape due to the diminished ability to automate the scraping process.\\nWebsites can declare if crawling is allowed or not in the robots.txt file and allow partial access, limit the crawl rate, specify the optimal time to crawl and more.\\nLoad database data straight into the HTML DOM via AJAX, and use DOM methods to display it, forcing crawlers to either reproduce those AJAX requests or use browser rendering (e.g. a headless browser).\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf83c3bb-3f75-41c0-b390-dc6bd029ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_3_ANS  :- Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
      " The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML in the document with which we’re working.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_3_ANS  :- Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\\n The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML in the document with which we’re working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176e5f47-26b7-4bf1-8887-1cb84c2aeafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_4_ANS :- Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library. \n"
     ]
    }
   ],
   "source": [
    "print(\"Q_4_ANS :- Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b89827-c185-4481-97f6-099303a5b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_5_ANS :- 1. AWS Organizations\n",
      "2. Elastic Beanstalk\n",
      "3. CodePipeline\n",
      "1. AWS Organization :- \n",
      "AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.\n",
      "2. Elastic Beanstalk :- \n",
      "AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.\n",
      "3. CodePipeline :-\n",
      "AWS CodePipeline is integrated with a number of products and services. The following sections describe best practices and use cases for CodePipeline and these related products and services.\n",
      "A simple business use case for CodePipeline can help you understand ways you might implement the service and control user access. The use cases are described in general terms. They do not prescribe the APIs to use to achieve the results you want.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_5_ANS :- 1. AWS Organizations\\n2. Elastic Beanstalk\\n3. CodePipeline\\n1. AWS Organization :- \\nAWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.\\n2. Elastic Beanstalk :- \\nAWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.\\n3. CodePipeline :-\\nAWS CodePipeline is integrated with a number of products and services. The following sections describe best practices and use cases for CodePipeline and these related products and services.\\nA simple business use case for CodePipeline can help you understand ways you might implement the service and control user access. The use cases are described in general terms. They do not prescribe the APIs to use to achieve the results you want.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b86642-f1a2-4068-a734-622831d580a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
